# ğŸš€ Desafio Data Engineer â€“ Streaming de Bitcoin (Kraken)

![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)
![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-Streaming-black?logo=apachekafka)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange?logo=apachespark)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.8-green?logo=apacheairflow)
![Docker](https://img.shields.io/badge/Docker-Container-blue?logo=docker)
![Parquet](https://img.shields.io/badge/Format-Parquet-brightgreen)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow)

---

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um **pipeline de streaming de dados em tempo real** para coletar cotaÃ§Ãµes de Bitcoin da exchange **Kraken**, processar os eventos via **Apache Kafka** e **Apache Spark Structured Streaming**, e armazenar os dados de forma **particionada e otimizada em formato Parquet**, com **orquestraÃ§Ã£o via Apache Airflow**, tudo executando em ambiente **Docker**.

O objetivo vai alÃ©m do requisito mÃ­nimo do desafio, aplicando **boas prÃ¡ticas de engenharia de dados**, organizaÃ§Ã£o de cÃ³digo, escalabilidade e observabilidade.

---

## ğŸ§  Entendimento do Desafio Original

### Desafio Proposto
- Consumir dados em tempo real da API da Kraken
- Utilizar uma plataforma de streaming (Kafka ou Flink)
- Persistir os dados em arquivos `.csv`
- Utilizar Docker
- (Opcional) Orquestrar com Airflow

### DomÃ­nio do Problema
Criar um stream de dados que coleta preÃ§o do Bitcoin em tempo real e registra os dados em arquivos estruturados.

---

## ğŸ”„ Principais EvoluÃ§Ãµes em RelaÃ§Ã£o ao Desafio

### âŒ CSV â†’ âœ… Parquet
Embora o desafio solicite arquivos CSV, optamos por **Parquet**, pois:

- Ã‰ um **formato colunar**, ideal para analytics
- Melhor **compressÃ£o **
- Melhor performance em leitura
- PadrÃ£o amplamente utilizado em **Data Lakes**
- CompatÃ­vel com Spark, Athena, BigQuery, Presto, Trino, etc.

> ğŸ’¡ Em cenÃ¡rios reais de Engenharia de Dados, CSV Ã© geralmente evitado para grandes volumes e dados histÃ³ricos.

---

### ğŸ“‚ Estrutura de Dados Particionada
Os dados sÃ£o organizados seguindo boas prÃ¡ticas de Data Lake:

```text
data/
â””â”€â”€ KRAKEN/
    â””â”€â”€ year=2026/
        â””â”€â”€ month=1/
            â””â”€â”€ day=30/
                â””â”€â”€ hour=18/
                    â””â”€â”€ symbol=BTC_USD/
                        â””â”€â”€ part-*.parquet
```

Isso permite:
- Queries mais rÃ¡pidas
- Leitura seletiva por perÃ­odo
- Facilidade de integraÃ§Ã£o com ferramentas analÃ­ticas

---

## ğŸ— Arquitetura da SoluÃ§Ã£o

```text
Kraken API
   â”‚
   â–¼
Kafka Producer (Python)
   â”‚
   â–¼
Apache Kafka (Topic: kraken.trades)
   â”‚
   â–¼
Apache Spark Structured Streaming
   â”‚
   â–¼
Parquet (Data Lake)
   â”‚
   â–¼
OrquestraÃ§Ã£o via Apache Airflow

```
---

## ğŸ—‚ Estrutura do Projeto

```
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ kraken_spark_parquet_dag.py
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ KRAKEN/
â”‚       â””â”€â”€ year=YYYY/month=MM/day=DD/hour=HH/symbol=BTC_USD
â”‚
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ producer/
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ config.py
â”‚       â”‚   â”œâ”€â”€ kafka_producer.py
â”‚       â”‚   â”œâ”€â”€ kraken_client.py
â”‚       â”‚   â”œâ”€â”€ models.py
â”‚       â”‚   â””â”€â”€ service.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ kraken_to_parquet/
â”‚   â”‚       â”œâ”€â”€ config.py
â”‚   â”‚       â”œâ”€â”€ main.py
â”‚   â”‚       â”œâ”€â”€ reader.py
â”‚   â”‚       â”œâ”€â”€ schema.py
â”‚   â”‚       â”œâ”€â”€ transformer.py
â”‚   â”‚       â””â”€â”€ writer.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ§° Tecnologias Utilizadas
Python 3.10

Apache Kafka

Apache Spark 3.5 (Structured Streaming)

Apache Airflow 2.8

Docker & Docker Compose

Parquet + Snappy

Kraken API

---
## âš™ï¸ VariÃ¡veis de Ambiente

Crie um arquivo .env na raiz do projeto:

```env
# Kraken
KRAKEN_API_URL=https://api.kraken.com
KRAKEN_PAIR=BTC/USD
POLL_INTERVAL_SECONDS=5

# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC=kraken.trades

# Spark
SPARK_APP_NAME=KrakenKafkaToPARQUET
BASE_OUTPUT_PATH=/data
EXCHANGE=KRAKEN
SYMBOL=BTC_USD
```
---

## â–¶ï¸ Como Rodar o Projeto
1ï¸âƒ£ Subir os serviÃ§os
```
docker-compose up -d --build
```

2ï¸âƒ£ Kafka Producer

O producer inicia automaticamente e comeÃ§a a publicar trades no Kafka.

Logs:
```
docker logs -f kafka-producer
```

3ï¸âƒ£ Spark Streaming (manual)
```
docker exec -it spark-master \
/opt/spark/bin/spark-submit \
--master spark://spark-master:7077 \
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
/opt/spark/jobs/kraken_to_parquet/main.py
```

4ï¸âƒ£ Spark UI

Acesse:
```
http://localhost:4040
```

---

## â± OrquestraÃ§Ã£o com Airflow
Acessar o Airflow

```
http://localhost:8080
```


```
UsuÃ¡rio: admin
Senha: admin
```

DAG DisponÃ­vel

kraken_spark_streaming

Essa DAG:

Verifica serviÃ§os

Executa o Spark Streaming

Monitora o job

---

## ğŸ§ª OrganizaÃ§Ã£o e Qualidade

CÃ³digo modular e desacoplado

SeparaÃ§Ã£o clara de responsabilidades

ConfiguraÃ§Ãµes centralizadas

Docker como padrÃ£o de execuÃ§Ã£o

Formato de dados pronto para analytics

---

## âœ… CritÃ©rios de AvaliaÃ§Ã£o Atendidos

âœ” OrganizaÃ§Ã£o do projeto
âœ” Clareza no README
âœ” Uso de streaming real
âœ” Boas prÃ¡ticas de engenharia de dados
âœ” OrquestraÃ§Ã£o com Airflow
âœ” CÃ³digo legÃ­vel e extensÃ­vel


---
## ğŸ‘¨â€ğŸ’» Autor

Rafael Vinicius Brambilla Alves